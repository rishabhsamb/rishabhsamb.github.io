this course is primarily concerned with learning parametric approximations of data distributions (the course assumes that all learnable data is indeed coming from a distribution).

Inference in the context of generative models means to learn a joint distribution over the data. Inference queries on generative models that we care about include:

1. Density estimation: given a datapoint $x$, what is the probability $p_{\theta}(x)$ assigned by the model (i.e. probability it came from the data distribution)?

2. Sampling: How can we generate novel data from the model distribution such that the new data is distributed similarly to the underlying data distribution.

3. Unsupervised representation learning: How can we learn meaningful feature representations for a datapoint $x$. This allows us to understand more about the underlying distribution (this one feels vague).

## Autoregressive models

Assume access to a dataset $D$ of $n$-dimensional datapoints $x$. For simplicity assume $x \in \{0,1\}^n$
.

### Representation

By the chain rule of probability ($P(A \cap B) = P(B|A)\cdot P(A)$), we have that

$$ p(x) = \Pi_{i=1}^n p(x_1 | x_1, x_2 \dots x_{i-1}) $$

The dependence of $x_j$ on all the $x_i$ for $i < j$ can be visualized as a 'Bayesian network'. Since we make no conditional independence assumptions (no $x_i$ is known to be independent from some $x_j$ for $j < i$), the Bayesian network is considered 'autoregressive'. We can then say that the *distribution for the $i$-th random variable depends on the $i$ variables before it*, for some fixed ordering of the $n$ variables.

This framework allows to fully represent any possible distribution over $n$ variables, but the space required to specify all such probabilities would be $\Theta(2^n)$.

So one approach is to approximate the conditional $p(x_i|x_{<i})$ using a Bernoulli random variable who's mean is a function parametrized by the $i-1$ previous variables. This ends up looking like

$$ p_{\theta_i}(x_i|x_{<i}) = \text{Bern}(f_i(x_1,x_2 \cdots ,x_{i-1})) $$

Each approximate conditional $p_{\theta_i}$ is specified by the set of parameters $\theta_i$ its function $f_i$ has. The problem has thus been reduced to this set of $i$ functions $\{f_i | 1 \leq i \leq n\}$. An MLP approach with 1 hidden layer takes $\Theta(n^2d)$ space since there is an $\mathbb{R}^{d \times (i-1)}$ matrix for each $i$ from $1$ to $n$ (ends up simplifying to $d(n+1)(n)/2$).

### Neural Autoregressive Density Estimator (NADE)

[NADE](http://proceedings.mlr.press/v15/larochelle11a/larochelle11a.pdf) attempts to reduce this space complexity by sharing a single weight matrix for each conditional. The parameters for each of the $n$ MLPs are shared, and they each use successively larger parts of the matrix for their calculations $f_i$ uses $W[..i] \in \mathbb{R}^{d \times (i-1)}$ where $W[..i]$ is the leftmost submatrix with d rows and $i-1$ columns (this makes sense don't pretend it doesn't). This reduces the space complexity to $\Theta(dn + dn + n)$ ($W \in \Theta(dn)$, the final layer activations for each MLP is a vector in $\mathbb{R}^d$, and there are $n$ of them, and there are $n$ biases for each final layer as well).

To extend NADE to real-valued functions, [RNADE](https://arxiv.org/abs/1306.0186) attempts to use a mixture of $K$ Gaussians. Instead of learning a mean function for a Bernoulli, we learn $K$ means and $K$ variances for each of the $K$ Gaussian distributions for each $i \in [1,n]$. To make this efficient, they have $i$ learnable functions $g_i:\mathbb{R}^{i-1} \rightarrow \mathbb{R^{2K}}$ output $K$ (mean,variance) pairs for the Gaussians approximating the $i$-th conditional.

Since NADE fixes orderings of the Bayesian network, you can use an ensemble of NADE models with different orderings to get (better?) results. See [EoNADE](https://arxiv.org/abs/1310.1757).

